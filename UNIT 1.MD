# ğŸ§  Large Language Models (LLMs) â€” Overview

---

## 1. Definition

A **Language Model (LM)** is a probabilistic model of text that generates tokens one at a time, selecting each from a probability distribution over possible candidates.
The term **â€œlargeâ€** refers to the number of parameters in the model.

> ğŸ’¡ **Q&A:**
> ğŸŸ¦ **Q:** How do we affect the probability distribution?
> ğŸ’¬ **A:** We affect the probability distribution through **prompting** and **training**.

---

## 2. LLM Architectures

LLMs are typically based on the **Transformer** architecture and are composed of **Encoders** and/or **Decoders**.
Models can vary by size (number of parameters) or capabilities (e.g., embeddings vs. generation).

### ğŸ§© Encoder

Encoders convert a sequence of tokens into an **embedding** (vector representation).
![Encoder](/assets/images/immagine_2025-10-15_180714834.png)

> ğŸ’¡ **Q&A:**
> ğŸŸ¦ **Q:** What does a cosine distance of 0 indicate between two embeddings?
> ğŸ’¬ **A:** It means the embeddings are **identical in direction**, i.e., **highly similar** in meaning.

---

### ğŸ” Decoder

Decoders take a sequence of tokens and output the **next token**, designed primarily for **text generation**.
![Decoder](/assets/images/immagine_2025-10-15_181107207.png)

---

### ğŸ”„ Encoder + Decoder Example

![Encoder+Decoder](/assets/images/immagine_2025-10-15_181308276.png)

---

## 3. Prompting

**Prompting (In-Context Learning)** consists of providing the LLM with additional instructions to influence its probability distribution â€” eliciting particular response styles or reasoning behaviors.

### âœï¸ Techniques

1. **K-shot prompting** â€” Provide *K* examples within the context window.
2. **Chain of Thought (CoT)** â€” Divide the request into subtasks and show reasoning steps to improve accuracy.
3. **Least-to-Most Prompting** â€” Instruct the LLM to decompose the problem and solve from easy to complex.
4. **Step-Back Prompting** â€” Ask the LLM to identify high-level concepts before solving the task.
5. **Zero-Shot Chain of Thought** â€” Like CoT but without examples.

#### Prompt Format

Llama 2 prompts can be formatted like this:

```
<<s>
[INST]
  <<SYS>>
    {{system prompt}}
  <</SYS>>
    {{user_message}}
[/INST]
```

> ğŸ’¡ **Q&A:**
> ğŸŸ¦ **Q:** When is **Soft Prompting** ideal?
> ğŸ’¬ **A:** **Soft prompting** is ideal when **full fine-tuning is impractical** but customization is still needed.
> It learns a **small set of continuous embeddings** that guide model behavior **without modifying original parameters**, providing an efficient adaptation method requiring minimal compute.

---

### âš ï¸ Safety Issues

* **Prompt Injection:** Malicious inputs can attempt to override instructions or extract sensitive data.
* Proper safeguards are needed to ensure that the LLM doesnâ€™t leak private or confidential information.

---

## 4. Decoding

**Decoding** is the process of generating text token by token from the modelâ€™s probability distribution.

### ğŸ§® Methods

1. **Greedy Decoding:** Always select the token with the highest probability.
2. **Stochastic Decoding:** Randomly sample among the high-probability candidates for diversity.

### âš™ï¸ Key Parameters

* **Temperature:** Controls randomness â€” higher temperature flattens the distribution (more creative, less deterministic).
* **Top-P (Nucleus Sampling):** Select tokens until cumulative probability â‰¥ *P*.
* **Top-K:** Consider only the *K* highest-probability tokens.

> ğŸ’¡ **Q&A:**
> ğŸŸ¦ **Q:** How can we ensure identical outputs for all users when generating text?
> ğŸ’¬ **A:** Set a **fixed seed value**. If the seed is `None`, the model will **generate diverse, random responses**.

---

## 5. Training

Prompting alone may not be sufficient for **domain-specific** or **specialized** tasks â€” additional training or **fine-tuning** is often required.

![Table](/assets/images/immagine_2025-10-15_182525150.png)

---

## 6. Retrieval-Augmented Generation (RAG)

LLMs can suffer from **hallucination** â€” confidently producing incorrect or unfounded answers when lacking relevant training data.

### âœ… Evaluation Aspects

* **Attributability:** Can the response be traced back to a specific source?
* **Groundedness:** How well is the answer supported by retrieved documents?

### ğŸ” How RAG Works

RAG is a **non-parametric** approach that augments LLMs with external data sources.

1. **Ingestion:** Documents are chunked, embedded, and stored in a vector database.
2. **Retrieval:** Perform a **semantic search** based on the user prompt to find similar documents.
3. **Generation:** Provide the retrieved context to the LLM to generate a grounded response.

> ğŸ’¡ **Q&A:**
> ğŸŸ¦ **Q:** What type of model is RAG considered, and what is its main advantage?
> ğŸ’¬ **A:** **RAG** is **non-parametric**, meaning it doesnâ€™t retrain the model.
> It can **answer about any external corpus** by retrieving and grounding relevant documents.

---

## 6.5 ğŸ§  LLM Customization Overview

Training and adapting large language models (LLMs) can significantly impact performance, cost, and efficiency.

---

### âš™ï¸ Training Considerations

Training a large LLM **from scratch** is often impractical due to cost, data, and expertise requirements.

---

### ğŸ§© Practical Alternatives

1. **ğŸª¶ In-Context Learning (Few-Shot Prompting)**
2. **ğŸ¯ Fine-Tuning**
3. **ğŸ“š Retrieval-Augmented Generation (RAG)**

---

### ğŸ§­ Choosing the Right Approach

Follow this progression:

1. âœ… Start with **simple prompting**
2. â• Add **few-shot examples**
3. ğŸ” Introduce **RAG**
4. ğŸ§¬ **Fine-tune** if necessary
5. âš¡ Combine **RAG + fine-tuning** for best performance

---

## 7. Code Models

Models like **GitHub Copilot** and **Code Llama** are trained on code and natural-language comments.
Theyâ€™re widely used for **programming assistance** and **code completion**.

---

## 8. Multi-Modal Models

**Multi-modal models** are trained on multiple input types â€” text, images, or audio.

* **Autoregressive models** (e.g., DALLÂ·E) generate outputs token by token.
* **Diffusion models** generate outputs all at once â€” great for images, less for text.

---

## 9. Language Agents

Frameworks enable LLM-based **agents** capable of **tool use**, **reasoning**, and **environment interaction**.

### ğŸ§  Agent Frameworks

1. **ReAct:** Alternates between reasoning and action.
2. **ToolFormer:** Inserts tool calls during pre-training.
3. **Bootstrapped Reasoning:** Uses model-generated rationales for fine-tuning.

---

### âœ… Summary

| Concept                    | Purpose                             |
| -------------------------- | ----------------------------------- |
| **Prompting**              | Steer model behavior in-context     |
| **Decoding**               | Generate coherent text              |
| **Training / Fine-tuning** | Adapt to new domains                |
| **RAG**                    | Ground responses with external data |
| **Multi-modal**            | Combine multiple data types         |
| **Agents**                 | Enable reasoning and tool use       |

---

## âœ… Integrated Test Answers Recap

| Concept                  | Answer                                               |
| ------------------------ | ---------------------------------------------------- |
| Probability distribution | Modified with prompting & training                   |
| Cosine distance = 0      | Vectors identical in direction (highly similar)      |
| Seed behavior            | Fixed seed = identical outputs; None = random        |
| RAG nature               | Non-parametric; answers any corpus                   |
| Soft prompting           | Efficient for light customization without retraining |








