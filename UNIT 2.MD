# ‚òÅÔ∏è OCI Generative AI Service

The **OCI Generative AI Service** is a **fully managed service** that provides a selection of customizable **large language models (LLMs)** available via API for building AI-powered applications.

---

## üöÄ Main Features

* **Choice of Models:** High-performing pretrained models from *Meta* and *Cohere*.
* **Flexible Fine-Tuning:** Adapt models to domain-specific data.
* **Dedicated AI Clusters:** GPU-backed clusters hosting both fine-tuning and inference workloads.

---

## üó£Ô∏è Pretrained Foundational Chat Models

| Model                           | Provider | Context Window | Output Tokens | Description                                                                                                                      |
| ------------------------------- | -------- | -------------- | ------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| **command-r-plus**              | Cohere   | Up to 128K     | Up to 4K      | Powerful model for Q&A, information retrieval, and sentiment analysis. Higher cost and latency.                                  |
| **command-r-16k**               | Cohere   | Up to 16K      | Up to 4K      | Entry-level use cases. Optimized for speed and cost efficiency.                                                                  |
| **llama 3-70b-instruct / 405b** | Meta     | Up to 128K     | ‚Äî             | Instruction-tuned models for conversational AI. The 405B model is the largest and best suited for enterprise-scale applications. |

---

## üî° Embedding Models

Embedding models convert tokens into **numerical vector representations** for tasks like semantic search, classification, or clustering.

| Model                              | Provider | Dimensionality | Max Tokens | Language Support        | Notes                   |
| ---------------------------------- | -------- | -------------- | ---------- | ----------------------- | ----------------------- |
| **embed-english-v3.0**             | Cohere   | 1024           | 512        | English                 | High-quality embeddings |
| **embed-multilingual-v3.0**        | Cohere   | 1024           | 512        | 100+ languages          | Multilingual support    |
| **embed-english-light-v2.0**       | Cohere   | 1024           | 512        | English                 | Lightweight version     |
| **light (English / Multilingual)** | Cohere   | 384            | 512        | English or Multilingual | Compact and efficient   |

---

## üîç Semantic vs. Lexical Search

* **Lexical Search:** Focuses on matching **keywords**.
* **Semantic Search:** Focuses on **meaning**, using vector similarity rather than literal matches.

### üßÆ Semantic Similarity

Similarity between embeddings can be computed using:

* **Cosine similarity**
* **Dot product**

Numerically similar vectors correspond to semantically similar meanings.

### üß± Sentence Embeddings

Entire sentences can be embedded ‚Äî semantically similar sentences will have embeddings that are **close in vector space**.

---

### üß† Embedding Use Case (RAG Flow)

```mermaid
flowchart LR
    A[User Question] --> B[Vector Database VDB]
    B --> C[Retrieve Relevant Content]
    C --> D[Send Content to LLM]
    D --> E[LLM Generates Informed Answer]
    E --> A
```

---

## üß© Fine-Tuning

Fine-tuning improves model performance for **specific tasks** using **custom data**.

1. **T-Few (Cohere):** Efficient fine-tuning method that updates only a small subset of parameters.
2. **Vanilla Fine-Tuning:** Updates most or all layers for maximum adaptability.

---

## ‚öôÔ∏è AI Clusters

Dedicated **AI clusters** are GPU-based compute resources for **custom fine-tuning** and **inference**.

* Built on **exclusive RDMA cluster networks** for low-latency GPU communication.
* Each customer‚Äôs GPU pool is **isolated** for performance and security.

![Cluster](/assets/images/immagine_2025-10-16_093903827.png)

---

## üìä OCI Generative AI Dashboard

Accessible through the **Analytics & AI service**, the GenAI dashboard allows customization of model and embedding parameters.

### üîß Model Parameters

1. **Top P:** Controls cumulative probability cutoff for token sampling.
2. **Top K:** Limits sampling to top *K* most likely tokens.
3. **Temperature:** Adjusts creativity/randomness of generation.
4. **Preamble Override:** Provides initial context or style guidelines.
5. **Frequency Penalty:** Reduces repetition of frequent tokens.
6. **Presence Penalty:** Penalizes token reuse regardless of frequency.
7. **Max Tokens:** Limits response length.

### üìà Embedding Model Parameters

* **Truncate:** Manages token overflow by truncating input text.

---

## üß† Model Fine-Tuning Workflow

1. **Create a Fine-Tuned Model:**

   * Assign a name and select the base model.
   * Choose the fine-tuning method (e.g., *T-Few* or *Vanilla*).
   * Specify or create a dedicated AI cluster.

2. **Deploy an Endpoint:**

   * Define endpoint details for routing inference traffic.
   * Associate with the desired model and cluster.

---

## üß∞ Inference API Integration

The OCI Generative AI Service exposes an **Inference API** for programmatic access.

---

## ‚öôÔ∏è Configuration Setup (Shared)

Before performing inference or embedding tasks, configure your OCI client and endpoint.

```python
import oci

# Load basic configuration
CONFIG_PROFILE = "DEFAULT"
config = oci.config.from_file("~/.oci/config", CONFIG_PROFILE)

# Define the Generative AI inference endpoint
endpoint = "https://inference.generativeai.us-chicago-1.oci.oraclecloud.com"

# Create a Generative AI Inference client
generative_ai_inference_client = oci.generative_ai_inference.GenerativeAIInferenceClient(
    config=config,
    service_endpoint=endpoint
)
```

---

## üí¨ Inference API Demo

This example shows how to call a **chat model** using the OCI Generative AI Inference API.

```python
from oci.generative_ai_inference.models import ChatDetails

chat_detail = ChatDetails(
    model_id="cohere.command-r-plus",
    input="What is the capital of France?",
    temperature=0.7,
    max_tokens=200
)

chat_response = generative_ai_inference_client.chat(chat_detail)
print("Chat Result:\n", chat_response.data)
```

---

## üî° Embedding Models Demo

Embedding models convert text into numerical vectors for similarity search, clustering, or semantic retrieval.

```python
import oci

# Using the same configuration and client from above
inputs = [
    "What is the capital of France?",
    "What is the capital of Sweden?",
    "What is the capital of Canada?"
]

embed_text_detail = oci.generative_ai_inference.models.EmbedTextDetails()
embed_text_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(
    model_id="cohere.embed-english-v3.0"
)
embed_text_detail.inputs = inputs
embed_text_detail.truncate = "NONE"
embed_text_detail.compartment_id = "ocid1.compartment.oc1..aaaaaaaexamplecompartmentid"

# Generate embeddings
embed_text_response = generative_ai_inference_client.embed_text(embed_text_detail)

print("************************* Embed Texts Result *************************")
print(embed_text_response.data)
```

---

## üñ•Ô∏è Dedicated AI Cluster

(todo)

---

Would you like me to export this updated document as a **Markdown file (.md)** so you can integrate it directly into your repository or documentation system?

